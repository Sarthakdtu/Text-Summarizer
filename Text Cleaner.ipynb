{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize \n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from stop_words import get_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "apos_words ={\n",
    "\"aren't\" :'are not',\"can't\":\"cannot\",\"couldn't\":\"could not\",\"didn't\":\"did not\",\n",
    "\"doesn't\":\"does not\",\"don't\":\"do not\",\"hdn't\":\"had not\",\"hasn't\":\"has not\",\n",
    "\"haven't\":\"have not\",\"he'd\":\"he would\",\"he'll\":\"he will\",\"he's\":\"he is\",\n",
    "\"He'd\":\"He would\",\"He'll\":\"He will\",\"He's\":\"He is\",\n",
    "\"I'd\":\"I would\",\"I'll\":\"I will\",\"I'm\":\"I am\",\"I've\":\"I have\",\n",
    "\"isn't\":\"is not\",\"it's\":\"it is\",\"let's\":\"let us\",\"mustn't\":\"must not\",\"shan't\":\"shall not\",\n",
    "\"she'd\":\"she would\",\"she'll\":\"she will\",\"she's\":\"she is\",\"shouldn't\":\"should not\",\n",
    "\"She'd\":\"She would\",\"She'll\":\"She will\",\"She's\":\"She is\",\n",
    "\"that's\":\"that is\",\"there's\":\"there is\",\"they'd\":\"they had\",\"they'll\":\"they will\",\n",
    "\"they're\":\"they are\",\"they've\":\"they have\",\"we'd\":\"we would\",\"we're\":\"we are\",\n",
    "\"we've\":\"we have\",\"weren't\":\"were not\",\"what'll\":\"what will\",\"what're\":\"what are\",\n",
    "\"what's\":\"what is\",\"what've\":\"what have\",\"where's\":\"where is\",\"who'd\":\"who would\",\n",
    "\"who'll\":\"who will\",\"who're\":\"who are\",\"who's\":\"who is\",\"who've\":\"who have\",\n",
    "\"won't\":\"will not\",\"wouldn't\":\"would not\",\"you'd\":\"you would\",\"you'll\":\"you will\",\n",
    "\"you're\":\"you are\",\"you've\":\"you have\", \"You're\":\"You are\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = list(string.punctuation)\n",
    "punc.append('--')\n",
    "punc.append('//')\n",
    "punc.append('â€™')\n",
    "punc.append('!!')\n",
    "punc.append(\"``\")\n",
    "punc.append(\"''\")\n",
    "punc.remove(\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['Danish','Dutch','English','Finnish','French','German',\n",
    "             'Hungarian','Italian','Portuguese','Russian','Spanish','Swedish']\n",
    "stop_words=[]\n",
    "for language in languages:\n",
    "    stop_words_pre = get_stop_words(language.lower())\n",
    "    for sw in stop_words_pre:\n",
    "        stop_words.append(sw) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_remover(words):\n",
    "    exp = '\\d+'\n",
    "    for word in words:\n",
    "        if re.match(exp, word):\n",
    "            words.remove(word)\n",
    "    return words\n",
    "         \n",
    "def apost_word_replacer(words):\n",
    "    for word in words:\n",
    "        if word in apos_words.keys():\n",
    "            w = apos_words[word]\n",
    "            idx = words.index(word)\n",
    "            words[idx] = w\n",
    "        elif re.match(\"\\w+'s\",word):\n",
    "            w = word.split(\"'\")\n",
    "            w = w[0]+\" is\"\n",
    "            idx = words.index(word)\n",
    "            words[idx] = w\n",
    "            \n",
    "            \n",
    "    return words        \n",
    "\n",
    "def stemmer(words):\n",
    "    lem =PorterStemmer()\n",
    "    for word in words:\n",
    "        w = lem.stem(word)\n",
    "        idx = words.index(word)\n",
    "        words[idx] = w\n",
    "    return words   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_modifier(words):    \n",
    "    words = digit_remover(words)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    new_text=\"\"\n",
    "    text_words = apost_word_replacer(text.split(\" \"))\n",
    "    for word in text_words:        \n",
    "        new_text =new_text+\" \"+word\n",
    "    text_words = word_tokenize(new_text)\n",
    "    text_words = text_modifier(text_words)\n",
    "    new_text=\"\"\n",
    "    for word in text_words:\n",
    "        if word in punc:\n",
    "            pass\n",
    "        else:\n",
    "            new_text =new_text+\" \"+word\n",
    "    return new_text        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_texts_cleaner(text_set):\n",
    "    all_texts=[]\n",
    "    for text in text_set:\n",
    "        new_text = text_cleaner(text)\n",
    "        all_texts.append(new_text)\n",
    "    return all_texts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_without_stop_words_aux(text):\n",
    "    words = word_tokenize(text)\n",
    "   # words = stemmer(words)\n",
    "    for word in words:\n",
    "        if word in stop_words:\n",
    "            idx = words.index(word)\n",
    "            words[idx] = \"\"\n",
    "        else:\n",
    "            idx = words.index(word)\n",
    "            words[idx] = word.lower()\n",
    "    new_text=\"\"        \n",
    "    for word in words:        \n",
    "        new_text =new_text+\" \"+word        \n",
    "    return new_text       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_without_stop_words(texts):\n",
    "    texts_ws = []\n",
    "    texts = all_texts_cleaner(texts)\n",
    "    for text in texts:\n",
    "        text = text_without_stop_words(text)\n",
    "        texts_ws.append(text)\n",
    "    return texts_ws    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
